# flash-attention

Flash Attention implemented in PyTorch, Numba, CUDA, JAX, Triton 

References:
- [ELI5: FlashAttention - Aleksa GordiÄ‡](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)
- [shreyansh26/FlashAttention-PyTorch](https://github.com/shreyansh26/FlashAttention-PyTorch/blob/master/flash_attention_causal.py)
- [tspeterkim/flash-attention-minimal](https://github.com/tspeterkim/flash-attention-minimal)

## Citations
```
@inproceedings{dao2022flashattention,
  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}
@inproceedings{dao2023flashattention2,
  title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}
```
